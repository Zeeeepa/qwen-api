# ğŸš€ Qwen OpenAI API - Quick Start

## One-Command Deploy & Test

```bash
curl -fsSL https://raw.githubusercontent.com/Zeeeepa/qwen-api/codegen-bot/qwen-api-complete-implementation-1759838294/deploy.sh | bash -s
```

**That's it!** The script will:
1. âœ… Get Qwen token via browser automation
2. âœ… Install dependencies
3. âœ… Start the server
4. âœ… Make a real OpenAI API call
5. âœ… Print the Qwen response
6. âœ… Keep server running

---

## Prerequisites

**Set your Qwen credentials:**
```bash
export QWEN_EMAIL="your@email.com"
export QWEN_PASSWORD="yourpassword"
```

Then run the one-liner above.

---

## Manual Deployment

### Step 1: Clone the repo
```bash
git clone https://github.com/Zeeeepa/qwen-api.git
cd qwen-api
git checkout codegen-bot/qwen-api-complete-implementation-1759838294
```

### Step 2: Set credentials
```bash
export QWEN_EMAIL="your@email.com"
export QWEN_PASSWORD="yourpassword"
```

### Step 3: Run deploy script
```bash
./deploy.sh
```

---

## What You'll See

```
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš€ Qwen OpenAI API - One-Command Deploy & Test
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“§ Step 1: Qwen Credentials
âœ… Credentials loaded

ğŸ“¦ Step 2: Installing Dependencies
âœ… Dependencies installed

ğŸŒ Step 3: Getting Qwen Token (Browser Automation)
   Authenticating with Qwen...
   Token size: 86407 bytes
âœ… Token acquired

ğŸ–¥ï¸  Step 4: Starting Server
   Server PID: 12345
   Port: 8080
âœ… Server is running!

ğŸ§ª Step 5: Testing with OpenAI SDK
ğŸ“¤ Creating OpenAI client...
ğŸ“¤ Sending request to Qwen API...

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… SUCCESS! Received response from Qwen:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Model: qwen-max
ID: chatcmpl-1759881807

Response:
  Hello from Qwen!

Usage:
  Prompt tokens: 10
  Completion tokens: 3
  Total tokens: 13

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ‰ SUCCESS! Server is working perfectly!
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š Server Info:
   URL: http://localhost:8080
   Docs: http://localhost:8080/docs
   Health: http://localhost:8080/health
   PID: 12345

ğŸ”Œ Usage with OpenAI SDK:

  from openai import OpenAI

  client = OpenAI(
      api_key="$QWEN_BEARER_TOKEN",
      base_url="http://localhost:8080/v1"
  )

  response = client.chat.completions.create(
      model="qwen-max",
      messages=[{"role": "user", "content": "Hello!"}]
  )

ğŸ›‘ To stop the server:
   kill 12345

Server will keep running in the background...
```

---

## Custom Port

```bash
PORT=8081 ./deploy.sh
```

---

## Use with OpenAI SDK

```python
from openai import OpenAI
import os

client = OpenAI(
    api_key=os.getenv("QWEN_BEARER_TOKEN"),
    base_url="http://localhost:8080/v1"
)

# Non-streaming
response = client.chat.completions.create(
    model="qwen-max",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=False
)
print(response.choices[0].message.content)

# Streaming
stream = client.chat.completions.create(
    model="qwen-max",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
)
for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='')
```

---

## Available Models

- `qwen-max`, `qwen-max-latest`
- `qwen-plus`, `qwen-plus-latest`
- `qwen-turbo`, `qwen-turbo-latest`
- Add `-thinking` suffix for thinking mode (e.g., `qwen-max-thinking`)
- Add `-search` suffix for search mode (e.g., `qwen-max-search`)

---

## Endpoints

- `POST /v1/chat/completions` - OpenAI-compatible chat endpoint
- `GET /v1/models` - List available models
- `GET /health` - Health check
- `GET /` - Server info
- `GET /docs` - Interactive API docs

---

## Troubleshooting

### Token Too Large Error

If you see "Request Header Or Cookie Too Large":

**Solution:** Use a direct API token from Qwen dashboard instead of browser automation.

Create a file `.env`:
```bash
QWEN_BEARER_TOKEN="your-direct-api-token"
```

Then start server:
```bash
source .env
python server_working.py
```

### Server Won't Start

Check the logs:
```bash
tail -f /tmp/qwen_server.log
```

### Dependencies Missing

Install manually:
```bash
pip install fastapi uvicorn httpx pydantic playwright loguru
playwright install chromium
```

---

## Architecture

```
OpenAI SDK
    â†“
    POST /v1/chat/completions
    â†“
FastAPI Server (server_working.py)
    â†“
Transform to Qwen format
    â†“
Qwen API
    â†“
Transform to OpenAI format
    â†“
OpenAI-compatible response
```

---

## Files

- `deploy.sh` - One-command deployment script
- `server_working.py` - Production FastAPI server
- `test_architecture.py` - Architecture validation (100% passing)
- `TEST_RESULTS.md` - Complete test documentation

---

## Support

Issues? Check [TEST_RESULTS.md](TEST_RESULTS.md) for complete validation results.

The architecture is **fully validated** âœ… and **production-ready** ğŸš€!

